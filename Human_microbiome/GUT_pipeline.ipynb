{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qiime2\n",
    "from tempfile import mkdtemp\n",
    "from qiime2.plugins import demux, deblur, quality_filter, \\\n",
    "                           metadata, feature_table, alignment, \\\n",
    "                           phylogeny, diversity, emperor, feature_classifier, \\\n",
    "                           taxa, composition\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from Bio import Entrez\n",
    "from pprint import pprint\n",
    "from sklearn.utils import shuffle\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from Bio import SeqIO\n",
    "import ast\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puliscel a working directory (result_extraction), elimina tutti i file e le cartelle fatta eccezione per il logfile\n",
    "# il quale viene solo svuotato\n",
    "def clean_workspace():\n",
    "    \n",
    "    # ci spostiamo nella working directory\n",
    "    starting_path = os.getcwd()\n",
    "    path = os.path.join(os.getcwd(), f'result_extraction')\n",
    "    os.chdir(path)\n",
    "    \n",
    "    # prendiamo tutti i file contenuti nella working directory e eliminiamo a meno che non sia il log file\n",
    "    files = os.listdir()\n",
    "    for file in files:\n",
    "        if file != 'sra_querying.log':\n",
    "            delete_command = f'rm -r {file}'\n",
    "            os.system(delete_command)\n",
    "            \n",
    "    # puliamo il log file\n",
    "    delete_old_log = 'cat > sra_querying.log'\n",
    "    os.system(delete_old_log)  \n",
    "    \n",
    "    # torniamo nella starting directory ./Human microbiome\n",
    "    os.chdir(starting_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCBI utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzioni per semplificare l'interrogazioen di NCBI\n",
    "Entrez.email = \"giacomo.villa.mi@gmail.com\"\n",
    "\n",
    "def good_print(text):\n",
    "    print(pprint(text))\n",
    "\n",
    "def esearch(db, query, num_max = 20):\n",
    "    handle = Entrez.esearch(db = db, term = query, retmax = num_max)\n",
    "    record = Entrez.read(handle, validate = True)\n",
    "    return record\n",
    "\n",
    "def esummary(db, id_val):\n",
    "    handle = Entrez.esummary(db = db, id = id_val)\n",
    "    record = Entrez.read(handle, validate = True)\n",
    "    return record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Sample generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prende in input il dataset completo, data la query, \n",
    "def write_age_mean(total, typology, experiment):\n",
    "    \n",
    "    man = total.query('sex == \"male\"')\n",
    "    woman = total.query('sex == \"female\"')\n",
    "    \n",
    "    mean_age_total = round(np.mean(total['age_years']), 4)\n",
    "    mean_age_man = round(np.mean(man['age_years']), 4)\n",
    "    mean_age_woman = round(np.mean(woman['age_years']), 4)\n",
    "    not_valid_sex = total.query(\"sex != 'female' and sex != 'male'\")\n",
    "    \n",
    "    if typology != 'sample':\n",
    "        logging.info(f'--------------------{typology.upper()}--------------------')\n",
    "    logging.info(f'Total number of {typology} people: {len(total)}')\n",
    "    logging.info(f'Total number of {typology} man: {len(man)}')\n",
    "    logging.info(f'Total number of {typology} woman: {len(woman)}')\n",
    "    logging.info(f'Total number of non valid sex {len(not_valid_sex)}')\n",
    "    \n",
    "    logging.info(f'Mean Age for total: {mean_age_total}')\n",
    "    logging.info(f'Mean Age for man: {mean_age_man}')\n",
    "    logging.info(f'Mean Age for woman: {mean_age_woman}')\n",
    "    logging.info(f'\\n')\n",
    "    \n",
    "    print(f'--------------------{typology.upper()}--------------------')\n",
    "    print(f'Total number of {typology} people: {len(total)}')\n",
    "    print(f'Total number of {typology} man: {len(man)}')\n",
    "    print(f'Total number of {typology} woman: {len(woman)}')\n",
    "    print(f'Total number of non valid sex {len(not_valid_sex)}')\n",
    "    \n",
    "    print(f'Mean Age for total: {mean_age_total}')\n",
    "    print(f'Mean Age for man: {mean_age_man}')\n",
    "    print(f'Mean Age for woman: {mean_age_woman}')\n",
    "    \n",
    "    starting_path = os.getcwd()\n",
    "    path = os.path.join(os.getcwd(), f'result_extraction/')\n",
    "    os.chdir(path)\n",
    "    directories = os.listdir()\n",
    "    if f'{experiment}' not in directories:\n",
    "        os.mkdir(f'{experiment}')\n",
    "        \n",
    "    os.chdir(f'{experiment}')\n",
    "    if typology != 'sample':\n",
    "        total.to_csv(f\"./dataset_query_result_{typology}.csv\", index=False, encoding='utf-8')\n",
    "    \n",
    "    os.chdir(starting_path)\n",
    "\n",
    "# Prende in input il dataset da cui campionare e gli index degli element già presi, finché non campiona qualcosa di \n",
    "# nuovo non termina\n",
    "def generate_single_sample(already_taken, from_df):\n",
    "    element = random.randint(0, len(from_df) - 1)\n",
    "    while (element in already_taken):\n",
    "        element = random.randint(0, len(from_df) - 1)\n",
    "    return element\n",
    "\n",
    "# Dato il dataset di partenza contenente solo elementi effettivamente presenti su NCBI\n",
    "# genera un nuovo dataset composto da n_samples elementi\n",
    "def get_final_sample(started_dataset, n_samples):\n",
    "    taken = set()\n",
    "    final_sample = pd.DataFrame(columns=started_dataset.columns)\n",
    "    for i in tqdm(range(n_samples), desc='Sampling data'):\n",
    "        new_sample = generate_single_sample(taken, started_dataset)\n",
    "        taken.add(new_sample)\n",
    "        final_sample = final_sample.append(started_dataset.iloc[new_sample], ignore_index=True)\n",
    "    return final_sample\n",
    "\n",
    "def write_not_valid_ids(not_valid_names):\n",
    "    files = os.listdir(f'./result_extraction')\n",
    "    \n",
    "    if 'not_valid_sample_names.csv' in files:\n",
    "        all_not_valid_names = pd.read_csv(\"./result_extraction/not_valid_sample_names.csv\", header=0, dtype=str)\n",
    "        elements = [all_not_valid_names, not_valid_names]\n",
    "        final_not_valid = pd.concat(elements, ignore_index=False, sort=False)\n",
    "        final_not_valid.to_csv(f\"./result_extraction/not_valid_sample_names.csv\", index=False)\n",
    "    else:\n",
    "        not_valid_names.to_csv(f\"./result_extraction/not_valid_sample_names.csv\", index=False)\n",
    "    \n",
    "# Dato il dataset di partenza con tutti gli elementi che rispettano la query posta al gut, tiene in considraz\n",
    "def sampling_data(start_dataset, typology, n_samples, experiment):\n",
    "    write_age_mean(start_dataset, typology, experiment)\n",
    "    valid_id = list()\n",
    "    not_valid_id = list()\n",
    "    for try_id in tqdm(start_dataset['sample_name'], desc='NCBI ids validation'):\n",
    "        handleSce = esearch('biosample', try_id)\n",
    "        if len(handleSce['IdList']) != 0:\n",
    "            valid_id.append(try_id)\n",
    "        else:\n",
    "            not_valid_id.append(str(try_id))\n",
    "            print(try_id)\n",
    "            \n",
    "    print(f'Total rows: {len(start_dataset)}')\n",
    "    print(f'Valid rows: {len(valid_id)}')\n",
    "    \n",
    "    not_valid_names = pd.DataFrame(data={\"not_valid_sample_name\": not_valid_id, \"typology\": [typology]*len(not_valid_id)})\n",
    "    print(len(not_valid_names))\n",
    "    write_not_valid_ids(not_valid_names)\n",
    "    \n",
    "    logging.info(f'Total rows: {len(start_dataset)}')\n",
    "    logging.info(f'Valid rows: {len(valid_id)}')\n",
    "            \n",
    "    valid_start_dataset = pd.DataFrame(columns=start_dataset.columns)\n",
    "    index = 0\n",
    "    for _, row in start_dataset.iterrows():\n",
    "        if row['sample_name'] in valid_id:\n",
    "            valid_start_dataset.loc[index] = row\n",
    "            index += 1\n",
    "            \n",
    "    valid_start_dataset = shuffle(valid_start_dataset)\n",
    "    final_sample = get_final_sample(valid_start_dataset, n_samples)\n",
    "    write_sample_info(final_sample, typology, experiment)\n",
    "\n",
    "# Funzione richiamata sul sample finale, traduce in csv il campione finale creando un csv di due colonne: sample_name,\n",
    "# typology (e.g. healthy/not_healthy), richiama write_age_mean per scrivere sul log file l'età media dei soggetti\n",
    "def write_sample_info(sample, typology, experiment):\n",
    "    man = sample.query(\"sex == 'male'\")\n",
    "    woman = sample.query(\"sex == 'female'\")\n",
    "    write_age_mean(sample, 'sample', experiment)\n",
    "    \n",
    "    sample = sample[['sample_name']]\n",
    "    sample['typology'] = [typology]*len(sample)\n",
    "    \n",
    "    files = os.listdir(f'./result_extraction/{experiment}')\n",
    "    \n",
    "    if f'final_sample_{experiment}.csv' in files:\n",
    "        final_sample = pd.read_csv(f\"./result_extraction/{experiment}/final_sample_{experiment}.csv\", header=0, dtype=str)\n",
    "        final_sample = final_sample[['sample_name', 'typology']]\n",
    "        elements = [final_sample, sample]\n",
    "        final_sample = pd.concat(elements, ignore_index=False, sort=False)\n",
    "        final_sample.to_csv(f\"./result_extraction/{experiment}/final_sample_{experiment}.csv\", index=False)\n",
    "    else:\n",
    "        sample.to_csv(f\"./result_extraction/{experiment}/final_sample_{experiment}.csv\", index=False)\n",
    "        \n",
    "    close_dashes = '-'*len(typology.upper())\n",
    "    logging.info(f'--------------------{close_dashes}--------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SRA operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione controller, prende in input il nome dell'esperimento (eg. healthy vs not_healthy) e le tipologie di campione\n",
    "# (e.g. healthy e not_healty), chiama la funzione che interroga SRA di NCBI, in seguito richiama la funzione per \n",
    "# concatenare i file fasta e infine la funzione per prendere le sequenze che ricorrono più spesso. Infine salva su un\n",
    "# file csv i fasta delle sequenze più popolose aggiungendo il campo che si rifà all'id di Biosample.\n",
    "# Il file csv delle frequenze più popole è l'input per blast\n",
    "def sra_querying(experiment, types):\n",
    "    \n",
    "    # Legge il csv contente i 30 campioni di una tipologia e i 30 campioni dell'altra tipologia dato l'esperimento\n",
    "    # (e.g. esperimento: healthy vs not_healthy estrare il csv che contiene i 30 sample_name degli healthy e i \n",
    "    # 30 sample_name dei not_healthy)\n",
    "    final_sample = pd.read_csv(f\"./result_extraction/{experiment}/final_sample_{experiment}.csv\", header=0, dtype=str)\n",
    "    \n",
    "    # crea una lista che conterrà, dati i record, il corrispettivo id di Biosample\n",
    "    bio_sample_id = list()\n",
    "    \n",
    "    sra_ids = list()\n",
    "    \n",
    "    # per ogni riga del csv dei 30 campioni di una tipologia e i 30 dell'altra tipologia dato l'esperimento\n",
    "    for index, row in final_sample.iterrows():\n",
    "        print(f'File number: {index+1}')\n",
    "        \n",
    "        # Gestione del problema sulla lettura di un sample_name con la concatenazione della stringa '001'\n",
    "        # in generale estrae l'input per la funzione che farà la query su SRA\n",
    "        record_id = str(row[0])[0:15]\n",
    "        record_typology = row[1]\n",
    "        \n",
    "        # data la singola interrogazione, aggiunge alla lista degli id di bio_sample l'id.\n",
    "        query_result = get_sequences(record_id, record_typology, experiment)\n",
    "        bio_sample_id.append(query_result[0])\n",
    "        sra_ids.append(query_result[1])\n",
    "        \n",
    "    # una volta scaricate tutti i file fasta data l'esperimento, per ogni tipologia (e.g healthy/not_healthy) \n",
    "    # crea un unico file con tutte le sequenze e poi prende, da questo file, solo quelle più popolose\n",
    "    for typology in types:\n",
    "        concatenate_fast_file(typology, 'fasta', experiment)\n",
    "        get_top_sequences(typology, experiment)\n",
    "        \n",
    "    # crea una nuova colonna dove, per ogni sample_name, vi sarà l'id di biosample associato e salva il nuovo csv\n",
    "    final_sample['bio_sample_id'] = bio_sample_id\n",
    "    final_sample['runId'] = sra_ids\n",
    "    final_sample.to_csv(f\"./result_extraction/{experiment}/final_sample_{experiment}.csv\", index=False)\n",
    "    \n",
    "    get_sample_top_sequences_count(experiment, types[0])\n",
    "    get_sample_top_sequences_count(experiment, types[1])\n",
    "    \n",
    "# Funzione che interroga SRA, dato il sample_name. Richiede anche la tipologia del campione (e.g. healthy o not_healthy)\n",
    "# e il nome dell'esperimento (e.g. healthy_vs_not_healthy), per andare a salvare correttamente nelle cartelle facenti\n",
    "# riferimento all'esperimento\n",
    "def get_sequences(sample_name, typology, experiment):\n",
    "    \n",
    "    # in funzione della tipologia del campione (e.g. healthy o not_healthy) e dell'esperimento (e.g. healthy_vs_not_healthy)\n",
    "    # definisce il path corretto dove andare a salvare il risultato\n",
    "    path = f'\"./result_extraction/{experiment}/SRA_{typology}\" '\n",
    "    command1 = f'fastq-dump --fasta --readids --outdir {path}'\n",
    "    command2 = f'fastq-dump --readids --outdir {path}'\n",
    "        \n",
    "    # Query su SRA e print utili\n",
    "    print(f'Sample id: {sample_name}')\n",
    "    handleSce = esearch('biosample', sample_name)\n",
    "    biosampleId = handleSce['IdList'][0]\n",
    "    print(f'Biosample ID {biosampleId}')\n",
    "    print(f'Typology: {typology}')\n",
    "    handleSra = Entrez.efetch(db='biosample', id=biosampleId, retmode='xml')\n",
    "    root = ET.fromstring(handleSra.read())\n",
    "    identifier = root.findall('.//BioSample//Ids//Id')\n",
    "    for i in identifier:\n",
    "        if i.attrib['db'] == 'SRA':\n",
    "            sraId = i.text\n",
    "    handleSra = Entrez.esearch(db='sra', term=sraId)\n",
    "    resultsSra = Entrez.read(handleSra)['IdList']\n",
    "    run_ids = list()\n",
    "    for s in resultsSra:\n",
    "        handlesngSraId = Entrez.efetch(db='sra', id=s, retmode='xml')\n",
    "        root = ET.fromstring(handlesngSraId.read())\n",
    "        identifier = root.find('.//EXPERIMENT_PACKAGE//RUN_SET//RUN')\n",
    "        runId = identifier.attrib['accession']\n",
    "        os.system(f'{command1}{runId}')\n",
    "        os.system(f'{command2}{runId}') \n",
    "        print(f'Run ID: {runId}')\n",
    "        run_ids.append(runId)\n",
    "    print()\n",
    "    return [biosampleId, run_ids]\n",
    "        \n",
    "\n",
    "# Dato il risultato delle query su SRA, concatena i file fasta facenti riferimento a una certa tipologia di record\n",
    "# (e.g. healthy o not_healthy) dato un certo esperimento (e.g. healthy_vs_not_healthy)\n",
    "def concatenate_fast_file(typology, file_format, experiment):\n",
    "    \n",
    "    # Prende tutti i file data la tipologia del record (e.g. healthy o not_healthy) contenuti nella cartella dove,\n",
    "    # dato l'esperimento (e.g. healthy_vs_not_healthy), la query su SRA ha riposto i risultati\n",
    "    files = os.listdir(f'./result_extraction/{experiment}/SRA_{typology}')\n",
    "    \n",
    "    # concatenazione file fasta\n",
    "    compact_files = list()\n",
    "    for file in files:\n",
    "        if file_format in file:\n",
    "            f = open(f'./result_extraction/{experiment}/SRA_{typology}/{file}', \"r\")\n",
    "            compact_files.append(f.read())\n",
    "            f.close()\n",
    "    f = open(f'./result_extraction/{experiment}/SRA_{typology}/final_{file_format}_{typology}.{file_format}', 'w')\n",
    "    for file in compact_files:\n",
    "        f.write(file)\n",
    "    f.close()\n",
    "    \n",
    "    # Eliminazione dei \n",
    "    starting_path = os.getcwd()\n",
    "    path = os.path.join(os.getcwd(), f'result_extraction/{experiment}/SRA_{typology}')\n",
    "    os.chdir(path)\n",
    "    command = 'rm *[0-9].fasta'\n",
    "    #os.system(command) \n",
    "    os.chdir(starting_path)\n",
    "        \n",
    "def get_top_sequences(typology, experiment):\n",
    "    records = list(SeqIO.parse(f\"./result_extraction/{experiment}/SRA_{typology}/final_fasta_{typology}.fasta\", format=\"fasta\"))\n",
    "    print(f'Number of sequences for {typology}: {len(records)}')\n",
    "    logging.info(f'Number of sequences for {typology}: {len(records)}')\n",
    "    \n",
    "    sequences = dict()\n",
    "    for record in tqdm(records, desc='Compacting fasta'):\n",
    "        if record.seq in sequences:\n",
    "            sequences[record.seq][0] += 1\n",
    "        else:\n",
    "            sequences[record.seq] = [1, f'>{record.description}']\n",
    "    \n",
    "    print(f'Number of grouped sequences: {len(sequences)}')\n",
    "    logging.info(f'Number of grouped sequences: {len(sequences)}')\n",
    "    \n",
    "    sequences_ord = {k: v for k, v in sorted(sequences.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    cont = 0\n",
    "    f = open(f'./result_extraction/{experiment}/SRA_{typology}/top_sequences_{typology}.fasta', 'w')\n",
    "    \n",
    "    for element in sequences_ord:\n",
    "        if sequences_ord[element][0] >= 100:\n",
    "            f.write(f'{sequences_ord[element][1]} number of reps {sequences_ord[element][0]}')\n",
    "            f.write('\\n')\n",
    "            f.write(str(element))\n",
    "            f.write('\\n')\n",
    "            cont += 1\n",
    "    f.close()\n",
    "    \n",
    "    print(f'Number of taken sequences: {cont}')\n",
    "    logging.info(f'Number of taken sequences: {cont}')\n",
    "    print()\n",
    "    logging.info('\\n')\n",
    "    \n",
    "def get_sample_top_sequences_count(experiment, typology):\n",
    "    top_sequences = list(SeqIO.parse(f\"./result_extraction/{experiment}/SRA_{typology}/top_sequences_{typology}.fasta\", format=\"fasta\"))\n",
    "    os.mkdir(f'./result_extraction/{experiment}/SRA_{typology}/tmp')\n",
    "    \n",
    "    columns = ['sample_name']\n",
    "    for i in range(len(top_sequences)):\n",
    "        columns.append(f'seq {i+1}')\n",
    "    final_summary = pd.DataFrame(columns = columns)\n",
    "    \n",
    "    final_sample = pd.read_csv(f\"./result_extraction/{experiment}/final_sample_{experiment}.csv\", dtype=str)\n",
    "    target_sample = final_sample.query(f\"typology == '{typology}'\")\n",
    "\n",
    "    for index, row in target_sample.iterrows():\n",
    "\n",
    "        print(f'Sample number: {index+1}')\n",
    "        run_ids = ast.literal_eval(row['runId'])\n",
    "        typology = row['typology']\n",
    "        sample_name = row['sample_name']\n",
    "        compact_files = list()\n",
    "        \n",
    "        print(f'Sample name: {sample_name}')\n",
    "\n",
    "        for run_id in run_ids:\n",
    "            f = open(f'./result_extraction/{experiment}/SRA_{typology}/{run_id}.fasta', \"r\")\n",
    "            compact_files.append(f.read())\n",
    "            f.close()\n",
    "            print(f'Run ID: {run_id}')\n",
    "\n",
    "            f = open(f'./result_extraction/{experiment}/SRA_{typology}/tmp/{sample_name}.fasta', 'w')\n",
    "            for file in compact_files:\n",
    "                f.write(file)\n",
    "            f.close()\n",
    "\n",
    "        sample_sequences = list(SeqIO.parse(f\"./result_extraction/{experiment}/SRA_{typology}/tmp/{sample_name}.fasta\", format=\"fasta\"))\n",
    "\n",
    "        top_sequences_in_sample = dict()\n",
    "        for top_sequence in tqdm(top_sequences):\n",
    "            top_sequences_in_sample[top_sequence.description] = 0\n",
    "            for sample_sequence in sample_sequences:\n",
    "                if top_sequence.seq == sample_sequence.seq:\n",
    "                    top_sequences_in_sample[top_sequence.description] += 1\n",
    "        numbers = [sample_name]\n",
    "        for seq in top_sequences_in_sample:\n",
    "            numbers.append(top_sequences_in_sample[seq])\n",
    "        final_summary.loc[len(final_summary)] = numbers\n",
    "\n",
    "    index = 0\n",
    "    for column in final_summary.columns:\n",
    "        if column != 'sample_name':\n",
    "            sum_column = final_summary[column].sum()\n",
    "            real_value = int(top_sequences[index].description.split('number of reps')[1].strip())\n",
    "            if sum_column != real_value:\n",
    "                print(f'Problem to column: {column}')\n",
    "\n",
    "            index += 1    \n",
    "\n",
    "    final_summary.to_csv(f\"./result_extraction/{experiment}/{typology}_top_sequences_distribution.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiime2 operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_analysis(experiment, types):\n",
    "    files = os.listdir(f'./result_extraction/{experiment}')\n",
    "    \n",
    "    if not(f'seq_artifact_{experiment}.qza' in files):\n",
    "        manifest_operation(experiment, types)\n",
    "        artifact = qiime2.Artifact.import_data('SampleData[SequencesWithQuality]', f'./result_extraction/{experiment}/manifest.tsv',\n",
    "                                        view_type='SingleEndFastqManifestPhred33V2')\n",
    "        artifact.save(f'./result_extraction/{experiment}/seq_artifact_{experiment}.qza')\n",
    "    else:\n",
    "        artifact = qiime2.Artifact.load(f'./result_extraction/{experiment}/seq_artifact_{experiment}.qza')\n",
    "        \n",
    "    return artifact\n",
    "    \n",
    "    #demux_filter_stats = quality_filter.methods.q_score(artifact)\n",
    "    #filter_stats = metadata.visualizers.tabulate(demux_filter_stats.filter_stats.view(qiime2.Metadata))\n",
    "    #filter_stats.visualization\n",
    "\n",
    "\n",
    "\n",
    "# Questa funzione compatta i file dataset_query_result (sostanzialmente ciò che si estreva da GUT) e quindi crea\n",
    "# date le due tipologie (e.g. healthy e not_healthy) dato l'esperimento (not_healthy_vs_healthy) un singolo file\n",
    "# che sarà di fatto la somma dei due dataset di partenza. Ovviamente viene inserito un nuovo campo (nuova colonna)\n",
    "# dove si specifica la tipologia del record (e.g. healthy o not_healthy)\n",
    "def manifest_operation(experiment, typology):\n",
    "    typology_1 = pd.read_csv(f'./result_extraction/{experiment}/dataset_query_result_{typology[0]}.csv', dtype=str)\n",
    "    typology_2 = pd.read_csv(f'./result_extraction/{experiment}/dataset_query_result_{typology[1]}.csv', dtype=str)\n",
    "    \n",
    "    typology_1['typology'] = [typology[0]]*len(typology_1)\n",
    "    typology_2['typology'] = [typology[1]]*len(typology_2)\n",
    "    \n",
    "    frames = [typology_1, typology_2]\n",
    "    final_dataset = pd.concat(frames)\n",
    "    if len(final_dataset) == (len(typology_1) + len(typology_2)):\n",
    "        with open(f'./result_extraction/{experiment}/dataset_query_result_{experiment}.tsv', \"w\", newline='') as dataset_query_result:\n",
    "            tsv_output = csv.writer(dataset_query_result, delimiter='\\t')\n",
    "            \n",
    "            tsv_output.writerow(list(final_dataset.columns))\n",
    "            cont = 0\n",
    "            for _, row in final_dataset.iterrows():\n",
    "                tsv_output.writerow(list(row))\n",
    "                cont += 1\n",
    "        \n",
    "    add_id_sample(experiment)\n",
    "    create_manifest(experiment)\n",
    "\n",
    "# Questa funzione, dato l'experimenti (e.g. healthy_vs_not_healthy) crea il file manifest come mostrato nella pagina \n",
    "# https://docs.qiime2.org/2020.2/tutorials/importing/. \n",
    "def create_manifest(experiment):\n",
    "    \n",
    "    # leggiamo il file che contiene i sample_name campionati (il file con sample_name, typology, biosample_id, sra_id)\n",
    "    final_sample = pd.read_csv(f\"./result_extraction/{experiment}/final_sample_{experiment}.csv\", dtype=str)\n",
    "    manifest = list()\n",
    "    \n",
    "    # per ogni riga andiamo a copiare i file fastq e a inserirli in una nuova cartella con un nuovo nome\n",
    "    # sample_name.fasq, andando a compattare nel caso in cui a un sample_name fossero associati più file \n",
    "    # fastq\n",
    "    for index, row in final_sample.iterrows():\n",
    "        run_ids = ast.literal_eval(row['runId'])\n",
    "        sample_identificator = row['sample_identificator']\n",
    "        sample_name = row['sample_name']\n",
    "        typology = row['typology']\n",
    "        print(f'Sample identificator: {sample_identificator}')\n",
    "        print(f'Sample name: {sample_name}')\n",
    "        print(f'Typology: {typology}')\n",
    "        print(f'Run ids: {run_ids}')\n",
    "        print()\n",
    "        \n",
    "        directories = os.listdir(f'./result_extraction/{experiment}/SRA_{typology}')\n",
    "        if 'tmp_fastq' not in directories:\n",
    "            os.mkdir(f'./result_extraction/{experiment}/SRA_{typology}/tmp_fastq')\n",
    "        \n",
    "        compact_files = list()\n",
    "        for run_id in run_ids:\n",
    "            f = open(f'./result_extraction/{experiment}/SRA_{typology}/{run_id}.fastq', \"r\")\n",
    "            compact_files.append(f.read())\n",
    "            f.close()\n",
    "            \n",
    "            \n",
    "        f = open(f'./result_extraction/{experiment}/SRA_{typology}/tmp_fastq/{sample_identificator}.fastq', 'w')\n",
    "        for file in compact_files:\n",
    "            f.write(file)\n",
    "        f.close()\n",
    "        \n",
    "        # nel manifest appendiamo la riga 'sample_identificator, absolute_path_to_fastq'\n",
    "        manifest.append([sample_identificator, f'/result_extraction/{experiment}/SRA_{typology}/tmp_fastq/{sample_identificator}.fastq'])\n",
    "    \n",
    "    # scriviamo il manifest \n",
    "    with open(f'./result_extraction/{experiment}/manifest.tsv', \"w\",newline='') as manifest_file:\n",
    "        tsv_output = csv.writer(manifest_file, delimiter='\\t')\n",
    "        tsv_output.writerow(['sample-id', 'absolute-filepath'])\n",
    "        for line in manifest:\n",
    "            tsv_output.writerow([line[0], f'{os.getcwd()}{line[1]}'])\n",
    "            \n",
    "            \n",
    "def add_id_sample(experiment):\n",
    "    final_sample = pd.read_csv(f\"./result_extraction/{experiment}/final_sample_{experiment}.csv\", dtype=str)\n",
    "    sample_id = [f'Sample{i}' for i in range(len(final_sample))]\n",
    "    final_sample['sample_identificator'] = sample_id\n",
    "    final_sample.to_csv(f\"./result_extraction/{experiment}/final_sample_{experiment}.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il dataset gut\n",
    "df = pd.read_csv(\"./data/american_gut.txt\", delimiter=\"\\t\", dtype=str)\n",
    "\n",
    "# Sostituisce con NAN valori non validi\n",
    "df.replace(' ', np.nan, inplace=True)\n",
    "df.replace('Not provided', np.nan, inplace=True)\n",
    "df.replace('Unspecified', np.nan, inplace=True)\n",
    "\n",
    "# Elimina dalla working directory tutti i risultati dello scorso esperimento, ripulisce il logfile\n",
    "clean_workspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializza il log file\n",
    "logging.basicConfig(filename='./result_extraction/sra_querying.log', level=logging.INFO, format='%(message)s')\n",
    "today = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "logging.info(f'RUN TIME: {today}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    print(column, end = ', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Healthy vs not healthy study\n",
    "### Healthy extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estraiamo dal dataset i dati di interesse\n",
    "healthy = df.query(\"smoking_frequency == 'Never' and alcohol_frequency == 'Never'\")\n",
    "\n",
    "healthy = healthy.query(\"cancer == 'I do not have this condition'\")\n",
    "\n",
    "healthy['bmi'] = healthy['bmi'].apply(lambda x: float(x))\n",
    "healthy = healthy.query(\"bmi >= 18.5 and bmi <= 24.99\")\n",
    "\n",
    "healthy['age_years'] = healthy['age_years'].apply(lambda x: float(x))\n",
    "healthy = healthy.query(\"age_years >= 20 and age_years <= 50\")\n",
    "\n",
    "healthy = healthy.query(\"body_site == 'UBERON:feces'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampling_data(healthy, 'healthy', 40, 'healthy_vs_not_healthy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not healthy extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_healthy = df.query(\"smoking_frequency == 'Occasionally (1-2 times/week)' or smoking_frequency == 'Daily' or smoking_frequency == 'Regularly (3-5 times/week)'\")\n",
    "not_healthy = not_healthy.query(\"alcohol_frequency == 'Occasionally (1-2 times/week)' or alcohol_frequency == 'Daily' or alcohol_frequency == 'Regularly (3-5 times/week)'\")\n",
    "\n",
    "not_healthy = not_healthy.query(\"cancer == 'I do not have this condition'\")\n",
    "\n",
    "not_healthy['bmi'] = not_healthy['bmi'].apply(lambda x: float(x))\n",
    "not_healthy = not_healthy.query(\"bmi < 18.5 or bmi > 24.99\")\n",
    "\n",
    "not_healthy['age_years'] = not_healthy['age_years'].apply(lambda x: float(x))\n",
    "not_healthy = not_healthy.query(\"age_years >= 20 and age_years <= 50\")\n",
    "\n",
    "not_healthy = not_healthy.query(\"body_site == 'UBERON:feces'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_data(not_healthy, 'not_healthy', 40, 'healthy_vs_not_healthy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not healthy old vs not healthy young studies\n",
    "### Not healthy old extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_healthy = df.query(\"smoking_frequency == 'Occasionally (1-2 times/week)' or smoking_frequency == 'Daily' or smoking_frequency == 'Regularly (3-5 times/week)'\")\n",
    "not_healthy = not_healthy.query(\"alcohol_frequency == 'Occasionally (1-2 times/week)' or alcohol_frequency == 'Daily' or alcohol_frequency == 'Regularly (3-5 times/week)'\")\n",
    "\n",
    "not_healthy = not_healthy.query(\"cancer == 'I do not have this condition'\")\n",
    "\n",
    "not_healthy['bmi'] = not_healthy['bmi'].apply(lambda x: float(x))\n",
    "not_healthy = not_healthy.query(\"bmi < 18.5 or bmi > 24.99\")\n",
    "\n",
    "not_healthy['age_years'] = not_healthy['age_years'].apply(lambda x: float(x))\n",
    "not_healthy_old = not_healthy.query(\"age_years >= 40 and age_years <= 50\")\n",
    "\n",
    "not_healthy_old = not_healthy_old.query(\"body_site == 'UBERON:feces'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_data(not_healthy_old, 'not_healthy_old', 17, 'not_healthy_old_vs_not_healthy_young')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not healthy young extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_healthy_young = not_healthy.query(\"age_years >= 20 and age_years <= 30\")\n",
    "\n",
    "not_healthy_young = not_healthy_young.query(\"body_site == 'UBERON:feces'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_data(not_healthy_young, 'not_healthy_young', 14, 'not_healthy_old_vs_not_healthy_young')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mental illness vs food disorders\n",
    "### Mental illness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mental_illness = df.query(\"country_residence == 'United States'\")\n",
    "\n",
    "mental_illness = mental_illness.query(\"body_site == 'UBERON:feces'\")\n",
    "\n",
    "mental_illness = mental_illness.query(\"mental_illness == 'true' or mental_illness == 'Yes'\")\n",
    "\n",
    "mental_illness['age_years'] = mental_illness['age_years'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_data(mental_illness, 'mental_illness', 30, 'mental_ill_vs_food_dis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Food disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_disorders = df.query(\"country_residence == 'United States'\")\n",
    "\n",
    "food_disorders = food_disorders.query(\"body_site == 'UBERON:feces'\")\n",
    "\n",
    "food_disorders = food_disorders.query(\"mental_illness == 'false' or mental_illness == 'No'\")\n",
    "\n",
    "food_disorders['bmi'] =  food_disorders['bmi'].apply(lambda x : float(x))\n",
    "food_disorders = food_disorders.query(\"bmi < 18.5 or bmi > 24.99\")\n",
    "\n",
    "food_disorders = food_disorders.query(\"(fruit_frequency == 'Never' or fruit_frequency == 'Rarely (less than once/week)')\")\n",
    "\n",
    "food_disorders = food_disorders.query(\"exercise_frequency=='Rarely (a few times/month)' or exercise_frequency=='Never'\")\n",
    "\n",
    "food_disorders['age_years'] = food_disorders['age_years'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_data(food_disorders, 'food_disorders', 30, 'mental_ill_vs_food_dis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI Quering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sra_querying('healthy_vs_not_healthy', ['healthy', 'not_healthy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sra_querying('not_healthy_old_vs_not_healthy_young', ['not_healthy_old', 'not_healthy_young'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sra_querying('mental_ill_vs_food_dis', ['mental_illness', 'food_disorders'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sra_querying('cancer_and_cardiovascular', ['cancer', 'cardiovascular'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiime2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = quality_analysis('mental_ill_vs_food_dis', ['mental_illness', 'food_disorders'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demux_sequences = demux.visualizers.summarize(artifact)\n",
    "demux_sequences.visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = quality_analysis('healthy_vs_not_healthy', ['healthy', 'not_healthy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demux_sequences = demux.visualizers.summarize(artifact)\n",
    "demux_sequences.visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = quality_analysis('not_healthy_old_vs_not_healthy_young', ['not_healthy_old', 'not_healthy_young'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demux_sequences = demux.visualizers.summarize(artifact)\n",
    "demux_sequences.visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "artifact = quality_analysis('cancer_and_cardiovascular', ['cancer', 'cardiovascular'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demux_sequences = demux.visualizers.summarize(artifact)\n",
    "demux_sequences.visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project_bio",
   "language": "python",
   "name": "final_project_bio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
